\relax 
\citation{holliday2006epigenetics}
\citation{jaenisch2003epigenetic}
\citation{egger2004epigenetics}
\citation{esteller2008epigenetics}
\citation{cancer2012comprehensive}
\citation{tibshirani1996regression}
\citation{zou2005regularization}
\citation{li2008network}
\citation{li2010variable}
\citation{pan2010incorporating}
\citation{luo2012two}
\citation{kim2013network}
\citation{li2008network}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Related Work}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{background}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Linear regression}{5}}
\newlabel{eq:lin_reg}{{2.1}{5}}
\citation{hoerl1970ridge}
\citation{tibshirani1996regression}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Ordinary least squares estimation}{6}}
\newlabel{sec:olse}{{2.2}{6}}
\newlabel{eq:rss}{{2.2}{6}}
\newlabel{eq:beta_est}{{2.3}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Penalized regression}{6}}
\newlabel{sec:pen_reg}{{2.3}{6}}
\newlabel{eq:pen_reg}{{2.4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Ridge regression}{6}}
\newlabel{sec:ridge}{{2.3.1}{6}}
\newlabel{eq:ridge}{{2.5}{6}}
\citation{zou2005regularization}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Lasso}{7}}
\newlabel{sec:lasso}{{2.3.2}{7}}
\newlabel{eq:lasso}{{2.6}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Elastic Net}{7}}
\newlabel{sec:enet}{{2.3.3}{7}}
\newlabel{eq:enet}{{2.7}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Network-constrained regularization}{7}}
\citation{li2008network}
\citation{li2010variable}
\citation{pan2010incorporating}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Grace}{8}}
\newlabel{sec:grace}{{2.4.1}{8}}
\newlabel{eq:grace}{{2.8}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}aGrace}{8}}
\newlabel{sec:agrace}{{2.4.2}{8}}
\newlabel{eq:agrace}{{2.9}{8}}
\citation{luo2012two}
\citation{luo2012two}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}GBLasso}{9}}
\newlabel{sec:gblasso}{{2.4.3}{9}}
\newlabel{eq:gblasso_full}{{2.10}{9}}
\newlabel{eq:gblasso_simplified}{{2.11}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Linf and aLinf}{9}}
\@writefile{toc}{\contentsline {subsubsection}{Linf}{9}}
\newlabel{sec:linf}{{2.4.4}{9}}
\newlabel{eq:linf_pen}{{2.12}{9}}
\newlabel{eq:gblasso_constrained}{{2.13}{9}}
\citation{li2010variable}
\citation{kim2013network}
\citation{shen2012likelihood}
\newlabel{eq:linf_constrained}{{2.14}{10}}
\@writefile{toc}{\contentsline {subsubsection}{aLinf}{10}}
\newlabel{sec:alinf}{{2.4.4}{10}}
\newlabel{eq:alinf_pen}{{2.15}{10}}
\newlabel{eq:alinf_constrained}{{2.16}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}TTLP and LTLP}{10}}
\newlabel{eq:tlp_pen}{{2.17}{10}}
\citation{shen2012likelihood}
\@writefile{toc}{\contentsline {subsubsection}{Indicator functions}{11}}
\@writefile{toc}{\contentsline {subsubsection}{TTLP}{11}}
\newlabel{sec:ttlp}{{2.4.5}{11}}
\newlabel{eq:tlp}{{2.19}{11}}
\newlabel{eq:ttlp}{{2.20}{11}}
\@writefile{toc}{\contentsline {subsubsection}{LTLP}{11}}
\newlabel{sec:ltlp}{{2.4.5}{11}}
\newlabel{eq:ltlp}{{2.21}{11}}
\citation{grant2014cvx}
\citation{grant2008graph}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Implementation Details}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:implementation}{{3}{13}}
\newlabel{tab:method_impl}{{3}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Regression methods implementation details}}{14}}
\newlabel{eq:enet_simplified}{{3.1}{14}}
\newlabel{eq:alpha_l1_ratio}{{3.2}{14}}
\citation{li2008network}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Synthetic Dataset Generation}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:datagen}{{4}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Predictor network generation}{15}}
\newlabel{sec:pred_net}{{4.1}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Generation of predictor observations}{16}}
\newlabel{sec:obs_gen}{{4.2}{16}}
\newlabel{eq:pred_order}{{4.1}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Response variable generation}{16}}
\newlabel{eq:noise}{{4.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Primary simulation setups}{16}}
\newlabel{sec:prim_sim_setups}{{4.3.1}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Setup 1}{17}}
\newlabel{eq:setup1}{{4.3}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Setup 2}{17}}
\newlabel{eq:setup2}{{4.4}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Setup 3}{18}}
\newlabel{eq:setup3}{{4.5}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Setup 4}{18}}
\newlabel{eq:setup4}{{4.6}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Secondary simulation setups}{18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Composite Voting Regression}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:comp_reg}{{5}{19}}
\newlabel{eq:coef_matrix}{{5.1}{20}}
\newlabel{eq:frac_votes}{{5.2}{20}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Hyperparameter Tuning}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{tuning}{{6}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Traditional approaches}{21}}
\newlabel{sec:trad_tuning}{{6.1}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Grid search}{21}}
\newlabel{eq:mse}{{6.1}{21}}
\citation{james2013introduction}
\citation{james2013introduction}
\citation{kohavi1995study}
\@writefile{toc}{\contentsline {subsubsection}{The validation set approach}{22}}
\@writefile{toc}{\contentsline {subsubsection}{K-fold cross validation}{22}}
\newlabel{eq:cv}{{6.2}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Grid search minimizing the cross-validated MSE}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Orchestrated parameter tuning}{23}}
\newlabel{sec:orc_par_tun}{{6.2}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Context and motivation}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Inspiration}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Methodology}{24}}
\newlabel{sec:orc_meth}{{6.2.3}{24}}
\@writefile{toc}{\contentsline {subsubsection}{Iterative procedure}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Local search grid neighborhood, two tuning hyperparameters; $P_{t-1}$ is the parameter combination selected in iteration $t-1$, parameter values in the $P_t$ neighborhood are considered in iteration $t$}}{24}}
\newlabel{fig:orc_tun_search_grid}{{6.1}{24}}
\newlabel{target_vector}{{1}{25}}
\newlabel{cand_comb}{{2}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Orchestrated tuning abstract structure, three regression methods; $M1_{t-1}$ denotes the parameter estimates of method $1$ for iteration $t-1$; Each tuning iteration $t$ of a method uses the common input data and the parameter estimates of all other methods for the previous iteration}}{26}}
\newlabel{fig:orc_tun_struct}{{6.2}{26}}
\@writefile{toc}{\contentsline {subsubsection}{Choice of starting points for the search}{26}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{27}}
\citation{kim2013network}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Simulation studies}{29}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Simulation setup}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Hyperparameter search spaces}{29}}
\newlabel{tab:tuning_values}{{7.2}{30}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Hyperparameter search space for all regression methods}}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Model metrics}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Prediction evaluation metrics}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Variable selection metrics}{31}}
\newlabel{sec:varsel}{{7.3.2}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Correlation}{31}}
\newlabel{eq:correlation}{{7.1}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Sensitivity}{31}}
\newlabel{eq:sensitivity}{{7.2}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Specificity}{32}}
\newlabel{eq:specificity}{{7.3}{32}}
\@writefile{toc}{\contentsline {subsubsection}{Precision}{32}}
\newlabel{eq:precision}{{7.4}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}CV-MSE tuning}{32}}
\newlabel{sec:disc_cvmse_tun}{{7.4}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces CV-MSE tuning mean model metrics for 20 synthetic datasets}}{33}}
\newlabel{tab:met_cvmse}{{7.2}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces CV-MSE tuning mean model metrics with standard deviation error bars for 20 synthetic datasets}}{34}}
\newlabel{fig:met_cvmse}{{7.1}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Orchestrated tuning}{34}}
\newlabel{sec:disc_orc_tun}{{7.5}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Orchestrated tuning mean model metrics with standard deviation error bars for 20 synthetic datasets}}{35}}
\newlabel{fig:met_orchestrated}{{7.2}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces Orchestrated tuning mean model metrics for 20 synthetic datasets}}{36}}
\newlabel{tab:met_orctun}{{7.3}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Comparison of tuning approaches}{36}}
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces Comparison of mean model metrics by parameter tuning method}}{36}}
\newlabel{tab:met_comp}{{7.4}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Comparison of mean model metrics by parameter tuning method}}{37}}
\newlabel{fig:met_comp}{{7.3}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Optimal hyperparameter value selection}{38}}
\@writefile{toc}{\contentsline {subsubsection}{Lasso}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Lasso method distribution of optimal tuning parameters}}{38}}
\newlabel{fig:tun_lasso}{{7.4}{38}}
\@writefile{toc}{\contentsline {subsubsection}{Elastic Net}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Elastic Net method distribution of optimal tuning parameters}}{39}}
\newlabel{fig:tun_enet}{{7.5}{39}}
\@writefile{toc}{\contentsline {subsubsection}{Grace}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Grace method distribution of optimal tuning parameters}}{40}}
\newlabel{fig:tun_grace}{{7.6}{40}}
\@writefile{toc}{\contentsline {subsubsection}{aGrace}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces aGrace method distribution of optimal tuning parameters}}{41}}
\newlabel{fig:tun_agrace}{{7.7}{41}}
\@writefile{toc}{\contentsline {subsubsection}{GBLasso}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces GBLasso method distribution of optimal tuning parameters}}{42}}
\newlabel{fig:tun_gblasso}{{7.8}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Linf}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Linf method distribution of optimal tuning parameters}}{43}}
\newlabel{fig:tun_linf}{{7.9}{43}}
\@writefile{toc}{\contentsline {subsubsection}{aLinf}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces aLinf method distribution of optimal tuning parameters}}{44}}
\newlabel{fig:tun_alinf}{{7.10}{44}}
\@writefile{toc}{\contentsline {subsubsection}{TTLP and LTLP}{44}}
\bibstyle{unsrt}
\bibdata{references}
\@writefile{toc}{\contentsline {subsubsection}{Composite}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces Composite method distribution of optimal tuning parameters}}{45}}
\newlabel{fig:tun_composite}{{7.11}{45}}
\bibcite{holliday2006epigenetics}{1}
\bibcite{jaenisch2003epigenetic}{2}
\bibcite{egger2004epigenetics}{3}
\bibcite{esteller2008epigenetics}{4}
\bibcite{cancer2012comprehensive}{5}
\bibcite{tibshirani1996regression}{6}
\bibcite{zou2005regularization}{7}
\bibcite{li2008network}{8}
\bibcite{li2010variable}{9}
\bibcite{pan2010incorporating}{10}
\bibcite{luo2012two}{11}
\bibcite{kim2013network}{12}
\bibcite{hoerl1970ridge}{13}
\bibcite{shen2012likelihood}{14}
\bibcite{grant2014cvx}{15}
\bibcite{grant2008graph}{16}
\bibcite{james2013introduction}{17}
\bibcite{kohavi1995study}{18}
