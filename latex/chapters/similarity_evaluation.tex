\chapter{Method Similarity Evaluation}
As discussed in previous chapters, in the context of our research multiple different regression methods operate on common input data. The work shown in this chapter aims to determine to what extent the final models produced by the different regression approaches are similar to one another. We also look into whether the orchestrated hyperparameter tuning approach, discussed in Section \ref{sec:orc_par_tun}, successfully increases this similarity as is intended by design.

The similarity between two linear models is measured with the use of cosine similarity \cite{manning2008introduction}, described in the following section. To determine the overall similarity between two regression methods, we calculate the similarity between their final coefficient vectors for each of the synthetic datasets. Final refers to the models produced by fitting the given regression method on the complete training dataset with the optimal hyperparameter combination obtained from the tuning procedure.


\section{Cosine similarity}
Cosine similarity measures the similarity between two vectors $A$ and $B$ of equal length $n$. It is defined as shown in Equation \ref{eq:cos_sim} and produces values in the range $[-1,1]$. We used the implementation provided by Scikit-Learn through the cosine\_similarity function.
\begin{equation} \label{eq:cos_sim}
similarity = cos(\theta) = \frac{A \cdot B}{||A||_2||B||_2} = \frac{\sum_{i=1}^{n}A_iB_i}{\sqrt{\sum_{i=1}^{n}A_i^2}\sqrt{\sum_{i=1}^{n}B_i^2}}
\end{equation}


\section{Similarity for CV-MSE tuning} \label{sec:sim_cvmse}
Results from the regression method similarity analysis for all synthetic datasets and use of CV-MSE hyperparameter tuning is shown in Table \ref{tab:sim_cvmse} and Figure \ref{fig:sim_cvmse}. The following observations can be made from the obtained results:

\begin{itemize}
	\item The Lasso and Elastic Net methods produce very similar models. This is to be expected given that the parameter tuning of the Elastic Net commonly selects high values for the L1 ratio (Figure \ref{fig:tun_enet}). As a result, the L1 penalty is a major component of the total Elastic Net penalty.
	\item The GBLasso and Linf methods also show high similarity, which is unsurprising as the Linf method is derived from the GBLasso (see  \ref{sec:linf}).
	\item Our initial expectation was for the TTLP and LTLP methods to be very similar. Surprisingly, the LTLP method bears a stronger similarity to the Lasso and Elastic Net due to its use of the L1 penalty.
\end{itemize}

\input{tables/similarity_cvmse}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{cv_mse_similarities}
	\caption{CV-MSE tuning mean regression method similarities}
	\label{fig:sim_cvmse}
\end{figure}


\section{Similarity for Orchestrated tuning} \label{sec:sim_orctun}
Orchestrated tuning was performed with a subset of the regression methods, the selection of which is discussed in Section \ref{sec:disc_orc_tun}. The ensemble contained the Lasso, Elastic Net, Grace, GBLasso and Linf methods. The orchestrated tuning approach is initialized with parameter starting points obtained from the CV-MSE tuning approach. Results from the regression method similarity analysis for all synthetic datasets is shown in Table \ref{tab:sim_orctun} and Figure \ref{fig:sim_orctun}. 

The observations made in Section \ref{sec:sim_cvmse} regarding the methods contained in the orchestrated ensemble remain true. The Lasso and Elastic Net, as well as the GBLasso and Linf methods continue to produce similar coefficient estimates. 

The Elastic Net method shows consistent high similarity to all other regression methods in the ensemble. This could be due to the high number of hyperparameter combinations in its search grid providing a higher flexibility in comparison to the other methods.

\input{tables/similarity_orctun}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.70]{orchestrated_similarities}
	\caption{Orchestrated tuning mean regression method similarities}
	\label{fig:sim_orctun}
\end{figure}

Figure \ref{fig:conv_orctun} shows the distribution of iterations elapsed before the orchestrated tuning procedure converged for all synthetic datasets.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{tuning/convergence_statistics}
	\caption{Iterations elapsed before convergence for all synthetic datasets}
	\label{fig:conv_orctun}
\end{figure}


\section{Similarity comparison by tuning method}
The regression method similarities discussed in sections \ref{sec:sim_cvmse} and \ref{sec:sim_orctun} have been merged in Table \ref{tab:sim_comp}. Only the similarities regarding methods in the orchestrated tuning ensemble are retained from the CV-MSE results. This is done to enable a more visible comparison between similarities obtained through both hyperparameter tuning approaches.

The mean similarity between each pair of regression methods is increased in all of the cases. For some pairs, an increase of over 0.1 cosine similarity is visible. The overall mean similarity between different methods, which in the case of CV-MSE tuning is 0.836, reaches a value of 0.915 when orchestrated tuning is used. 

\input{tables/similarity_comparison}

The comparison between method similarities obtained in the context of CV-MSE and orchestrated hyperparameter tuning clearly shows a similarity increase when our proposed tuning approach is used. This indicates that the desired effect of improved cross-method consensus is achieved. 