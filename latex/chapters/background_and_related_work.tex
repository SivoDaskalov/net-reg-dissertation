This chapter briefly reviews concepts related to parametric estimation and linear regression. We describe in detail the various methods for penalized regression found in literature and used in this project.


\section{Linear Regression}
Let us consider an entity with a number of scalar measurable (observable) properties, for example temperature, weight, and size. We can define a matrix $X$ of $n$ rows and $p$ columns, such that each column contains the observed values of a particular property and each row represents an independent observation of values for all properties. Let us also define a list $y$ of length $n$ containing the corresponding observed values of an arbitrary property.

Linear regression is an approach for modeling the relationships between a scalar dependent (target) variable $y$ and a number of explanatory variables (predictors) $X_1,...,X_p$. It assumes that this relationship is linear and assigns a regression coefficient $p_i$ to each predictor $X_i$, as well as a constant (offset) term $\beta_0$. The linear regression model takes the form shown in \ref{eq:lin_reg}
\begin{equation} \label{eq:lin_reg}
y_i = \beta_01+\beta_1X_{i1}+\beta_2X_{i2}+...+\beta_pX_{ip}, \quad for\ i=1,2,...,n
\end{equation}


\section{Ordinary Least Squares Estimation}
Ordinary least squares (OLS) is a method of estimating the unknown parameters $\beta$ in a linear regression model. It aims to minimize the sum of squared deviations of the observed values from the model prediction, called residual sum of squares (RSS). The objective function of the minimization is shown on equation \ref{eq:rss}. 
\begin{equation} \label{eq:rss}
S(\beta) = \sum_{i=1}^{N} (y_i - x_i^T\beta)^2
\end{equation}

\begin{equation} \label{eq:beta_estimation}
\hat{\beta} = argmin_{\beta \in R}\ S(\beta) = \sum_{i=1}^{N} (y_i - x_i^T\beta)^2
\end{equation}

\section{Penalized Regression}


\section{Lasso}
least absolute shrinkage and selection operator
\begin{equation}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda_1\sum_{i=1}^{p}\left|\beta_i\right|
\end{equation}
$alpha = \lambda_1$


\section{Elastic Net}
\begin{equation}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda_1\sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2\sqrt{\sum_{i=1}^{p}\beta_i^2}
\end{equation}
$alpha = \lambda_1 + \lambda_2$ and $l1\_ratio = \lambda_1/(\lambda_1+\lambda_2)$


\section{Grace}
\begin{equation}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda_1\sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2\sum_{u \sim v}\left(\frac{\beta_u}{\sqrt{d_u}}-\frac{\beta_v}{\sqrt{d_v}}\right)^2w(u,v)
\end{equation}


\section{aGrace}
\begin{equation}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda_1\sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2\sum_{u \sim v}\left(\frac{sign(\tilde{\beta}_u)\beta_u}{\sqrt{d_u}}-\frac{sign(\tilde{\beta}_v)\beta_v}{\sqrt{d_v}}\right)^2w(u,v)
\end{equation}


\section{GBLasso} \label{sec:gblasso}
This method was initially suggested in \cite{pan2010incorporating}. Pan, Xie and Shen considered the objective function shown in equation \ref{eq:gblasso_full}. 
\begin{equation} \label{eq:gblasso_full}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda2^{1/\gamma'}\sum_{u \sim v}\left(\frac{|b_u|^\gamma}{w_u}-\frac{|b_v|^\gamma}{w_v}\right)^{1/\gamma},
\end{equation}
where $w_i$ is a weight function attributed to each node. Three types of functions, dependent on the node's degree $d_i$ and/or $\gamma$, were considered by the authors: $w_i = d_i^{(\gamma+1)/2}$, $w_i = d_i$ and $w_i = d_i^\gamma$.

A simplification of the penalty function is presented in \cite{kim2013network}. The authors have selected a node weight function of $w_i = d_i^{\gamma/2}$ and the penalty multiplier $\lambda2^{1/\gamma'}$ has been transformed to $\lambda$. The simplified objective function is shown in \ref{eq:gblasso_simplified}.
\begin{equation} \label{eq:gblasso_simplified}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + 
\lambda\sum_{u \sim v}
\left[\left(\frac{|b_u|}{\sqrt{d_u}}\right)^\gamma-
\left(\frac{|b_v|}{\sqrt{d_v}}\right)^\gamma\right]^{1/\gamma}
\end{equation}



\section{Linf and aLinf}
The authors of \cite{luo2012two} continued the study presented in section \ref{sec:gblasso}. They noted that as $\gamma\rightarrow\infty$ the penalty \ref{eq:gblasso_simplified} becomes \ref{eq:linf_full}.
\begin{equation} \label{eq:linf_full}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + 
\lambda\sum_{u \sim v}\max\left(\frac{|\beta_u|}{\sqrt{d_u}},\frac{|\beta_v|}{\sqrt{d_v}}\right)
\end{equation}

Luo, Pan and Shen also suggest an equivalent formulation of the penalized estimation in \ref{eq:gblasso_simplified} as the constrained minimization problem shown in \ref{eq:gblasso_constrained}.
\begin{align} \label{eq:gblasso_constrained}
\begin{split}
S(\beta) = &\sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \\ 
&subject\ to\ \sum_{u \sim v}
\left[\left(\frac{|b_u|}{\sqrt{d_u}}\right)^\gamma-
\left(\frac{|b_v|}{\sqrt{d_v}}\right)^\gamma\right]^{1/\gamma}\leq C
\end{split}
\end{align}

As previously shown, under $\gamma\rightarrow\infty$ equation \ref{eq:gblasso_constrained} transforms to \ref{eq:linf_constrained}. 

\begin{align} \label{eq:linf_constrained}
\begin{split}
S(\beta) = &\sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \\ 
&subject\ to\ \sum_{u \sim v}\max\left(\frac{|\beta_u|}{\sqrt{d_u}},\frac{|\beta_v|}{\sqrt{d_v}}\right)\leq C
\end{split}
\end{align}

The authors suggest an additional modification to reduce bias in the parameter estimates of the standard Linf method. They propose a two-step approach ...

\begin{equation} \label{eq:alinf_pen}
P(\beta) = \lambda\sum_{u \sim v}\left|\frac{sign(\tilde{\beta}_u)\beta_u}{\sqrt{d_u}}-\frac{sign(\tilde{\beta}_v)\beta_v}{\sqrt{d_v}}\right|
\end{equation}

Which produces the following constrained minimization problem:
\begin{align} \label{eq:alinf_constrained}
\begin{split}
S(\beta) = &\sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \\ 
&subject\ to\ \sum_{u \sim v}\left|\frac{sign(\tilde{\beta}_u)\beta_u}{\sqrt{d_u}}-\frac{sign(\tilde{\beta}_v)\beta_v}{\sqrt{d_v}}\right|\leq E
\end{split}
\end{align}


\section{TTLP and LTLP}


\newpage

A more extensive coverage of what's required to understand your 
work. In general you should assume the reader has a good undergraduate 
degree in computer science, but is not necessarily an expert in 
the particular area you've been working on. Hence this chapter 
may need to summarize some ``text book'' material. 

This is not something you'd normally require in an academic paper, 
and it may not be appropriate for your particular circumstances. 
Indeed, in some cases it's possible to cover all of the ``background'' 
material either in the introduction or at appropriate places in 
the rest of the dissertation. 

\newpage

This chapter covers relevant (and typically, recent) research 
which you build upon (or improve upon). There are two complementary 
goals for this chapter: 
\begin{enumerate} 
  \item to show that you know and understand the state of the art; and 
  \item to put your work in context
\end{enumerate} 

Ideally you can tackle both together by providing a critique of
related work, and describing what is insufficient (and how you do
better!)

The related work chapter should usually come either near the front or
near the back of the dissertation. The advantage of the former is that
you get to build the argument for why your work is important before
presenting your solution(s) in later chapters; the advantage of the
latter is that don't have to forward reference to your solution too
much. The correct choice will depend on what you're writing up, and
your own personal preference.