This chapter briefly reviews the main concepts of linear regression. We describe in detail the various methods for penalized regression found in literature and used in this project.


\section{Linear Regression}
Let us consider an entity with a number of scalar measurable (observable) properties, e.g. temperature, weight, dimensions. We can define a matrix $X$ of $n$ rows and $p$ columns, such that each column contains the observed values of a particular property and each row represents an independent observation of values for all properties. Let us also define a vector $y$ of length $n$ containing the corresponding observed values of an arbitrary property of interest.

Linear regression is a method for modeling the relationships between a scalar dependent (target) variable $y$ and a number of explanatory variables (predictors) $X_1,...,X_p$. It assumes that this relationship is linear and assigns a regression coefficient $\beta_i$ to each predictor $X_i$, as well as a constant (offset) term $\beta_0$. The linear regression model takes the form shown in \ref{eq:lin_reg}
\begin{equation} \label{eq:lin_reg}
y_i = \beta_01+\beta_1X_{i1}+\beta_2X_{i2}+...+\beta_pX_{ip}+\epsilon_i, \quad for\ i=1,2,...,n
\end{equation}
where $\epsilon_i$ represents noise, capturing all external factors influencing the target values, such as inaccuracy of measurement. The error $\epsilon_i$ introduces cannot be predicted or reduced.


\section{Ordinary Least Squares Estimation}
Ordinary least squares (OLS) is a method of estimating the unknown parameters $\beta$ in a linear regression model. It aims to minimize the sum of squared deviations of the observed values from the model prediction (\ref{eq:rss}), also called residual sum of squares (RSS).
\begin{equation} \label{eq:rss}
L(\beta) = \sum_{i=1}^{N} (y_i - x_i^T\beta)^2
\end{equation}
The parameter estimate $\hat{\beta}$ for the linear regression model is obtained as shown in equation \ref{eq:beta_est} through the minimization of the objective function $S(\beta)$.
\begin{equation} \label{eq:beta_est}
\hat{\beta} = argmin_{\beta \in R}\ S(\beta) = L(\beta)
\end{equation}


\section{Penalized Regression}
Penalized regression methods introduce a penalty $P(\beta)$ to the objective function $S(\beta)$ in addition to the loss function $L(\beta)$. $P$ penalizes values of the unknown parameters that are considered unrealistic in the current context, which is done to obtain a more meaningful estimation. One or more regularization parameters $\lambda_i$ can be used to balance the effect of any introduced penalties by scaling them. The general form of penalized regression is shown in equation \ref{eq:pen_reg}. 
\begin{equation} \label{eq:pen_reg}
S(\beta) = L(\beta) + P(\beta)
\end{equation}


\subsection{Ridge regression}
Ridge regression \cite{hoerl1970ridge}, also called Tikhonov or L2 regularization, is used to penalize large values in the $\beta$ estimate. The penalty, shown in equation \ref{eq:ridge}, causes the parameter estimates of the less important predictors to be shrinked, but remain non-zero. As a result, L2 regularization does not perform feature selection.
\begin{equation} \label{eq:ridge}
P(\beta) = \lambda\sqrt{\sum_{i=1}^{p}\beta_i^2}
\end{equation}


\subsection{Lasso}
The least absolute shrinkage and selection operator (LASSO) was introduced by Tibshirani in \cite{tibshirani1996regression}. It produces a sparse coefficient vector, whose remaining non-zero elements define a subset of the most relevant predictors. Model sparsity is especially important in high-dimensional problems, such as those arising when processing epigenetic data. The L1 penalty, shown in equation \ref{eq:lasso}, performs both variable selection and regularization.
\begin{equation} \label{eq:lasso}
P(\beta) = \lambda\sum_{i=1}^{p}\left|\beta_i\right|
\end{equation}


\subsection{Elastic Net}
The Elastic Net \cite{zou2005regularization}, suggested by Zou and Hastie, linearly combines the L1 (\ref{eq:lasso}) and L2 (\ref{eq:ridge}) penalties. This approach overcomes the individual limitations of the Lasso and Ridge methods. The elastic net penalty, shown in equation \ref{eq:enet}, is adjusted by two hyperparameters $\lambda_1$ and $\lambda_2$, one for each of the two penalty terms.
\begin{equation} \label{eq:enet}
P(\beta) = \lambda_1\sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2\sqrt{\sum_{i=1}^{p}\beta_i^2}
\end{equation}


\section{Network-constrained regularization}
Various approaches for network-constrained regularization have been developed in recent years. They enable the use of prior knowledge in the form of a network in the parameter estimation process. This allows methods to consider known relationships between predictors. In the context of epigenetic research, prior knowledge could be provided as a gene network representing known interactions between genes. Biological knowledge about the predictors should lead to a better understanding of the data and improved (biological) meaningfulness of the results.

For all network-constrained regularization approaches presented in this section, we define the following notation:\\
Let us consider a network that is represented by a weighted graph $G = (V, E, W)$, where $V$ is the set of vertices corresponding to the $p$ predictors, $E$ is the set of edges and $W$ contains their corresponding weights. An edge between the vertices $u$ and $v$ is represented as $u \sim v$ and its edge weight is $w(u,v)$. Let us define the degree $d_v$ of a vertex $v$ as $d_v = \sum_{u \sim v}w(u,v)$.

\subsection{Grace}
\cite{li2008network}
\begin{equation} \label{eq:grace}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda_1\sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2\sum_{u \sim v}\left(\frac{\beta_u}{\sqrt{d_u}}-\frac{\beta_v}{\sqrt{d_v}}\right)^2w(u,v)
\end{equation}


\subsection{aGrace} \label{eq:agrace}
\begin{equation}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda_1\sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2\sum_{u \sim v}\left(\frac{sign(\tilde{\beta}_u)\beta_u}{\sqrt{d_u}}-\frac{sign(\tilde{\beta}_v)\beta_v}{\sqrt{d_v}}\right)^2w(u,v)
\end{equation}


\subsection{GBLasso} \label{sec:gblasso}
This method was initially suggested in \cite{pan2010incorporating}. Pan, Xie and Shen considered the objective function shown in equation \ref{eq:gblasso_full}. 
\begin{equation} \label{eq:gblasso_full}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \lambda2^{1/\gamma'}\sum_{u \sim v}\left(\frac{|b_u|^\gamma}{w_u}+\frac{|b_v|^\gamma}{w_v}\right)^{1/\gamma},
\end{equation}
where $w_i$ is a weight function attributed to each node. Three types of functions, dependent on the node's degree $d_i$ and/or $\gamma$, were considered by the authors: $w_i = d_i^{(\gamma+1)/2}$, $w_i = d_i$ and $w_i = d_i^\gamma$.

A simplification of the penalty function is presented in \cite{luo2012two}. The authors have selected a node weight function of $w_i = d_i^{\gamma/2}$ and the penalty multiplier $\lambda2^{1/\gamma'}$ has been transformed to $\lambda$. The simplified objective function is shown in \ref{eq:gblasso_simplified}.
\begin{equation} \label{eq:gblasso_simplified}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + 
\lambda\sum_{u \sim v}
\left[\left(\frac{|b_u|}{\sqrt{d_u}}\right)^\gamma+
\left(\frac{|b_v|}{\sqrt{d_v}}\right)^\gamma\right]^{1/\gamma}
\end{equation}



\subsection{Linf and aLinf}
The authors of \cite{luo2012two} continued the study presented in section \ref{sec:gblasso}. They noted that as $\gamma\rightarrow\infty$ the penalty \ref{eq:gblasso_simplified} becomes \ref{eq:linf_full}.
\begin{equation} \label{eq:linf_full}
S(\beta) = \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + 
\lambda\sum_{u \sim v}\max\left(\frac{|\beta_u|}{\sqrt{d_u}},\frac{|\beta_v|}{\sqrt{d_v}}\right)
\end{equation}

Luo, Pan and Shen also suggest an equivalent formulation of the penalized estimation in \ref{eq:gblasso_simplified} as the constrained minimization problem shown in \ref{eq:gblasso_constrained}.
\begin{align} \label{eq:gblasso_constrained}
\begin{split}
S(\beta) = &\sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \\ 
&subject\ to\ \sum_{u \sim v}
\left[\left(\frac{|b_u|}{\sqrt{d_u}}\right)^\gamma+
\left(\frac{|b_v|}{\sqrt{d_v}}\right)^\gamma\right]^{1/\gamma}\leq C
\end{split}
\end{align}

As previously shown, under $\gamma\rightarrow\infty$ equation \ref{eq:gblasso_constrained} transforms to \ref{eq:linf_constrained}. 

\begin{align} \label{eq:linf_constrained}
\begin{split}
S(\beta) = &\sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \\ 
&subject\ to\ \sum_{u \sim v}\max\left(\frac{|\beta_u|}{\sqrt{d_u}},\frac{|\beta_v|}{\sqrt{d_v}}\right)\leq C
\end{split}
\end{align}

The authors suggest an additional modification to reduce bias in the parameter estimates of the standard Linf method. They propose a two-step approach ...

\begin{equation} \label{eq:alinf_pen}
P(\beta) = \lambda\sum_{u \sim v}\left|\frac{sign(\tilde{\beta}_u)\beta_u}{\sqrt{d_u}}-\frac{sign(\tilde{\beta}_v)\beta_v}{\sqrt{d_v}}\right|
\end{equation}

Which produces the following constrained minimization problem:
\begin{align} \label{eq:alinf_constrained}
\begin{split}
S(\beta) = &\sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \\ 
&subject\ to\ \sum_{u \sim v}\left|\frac{sign(\tilde{\beta}_u)\beta_u}{\sqrt{d_u}}-\frac{sign(\tilde{\beta}_v)\beta_v}{\sqrt{d_v}}\right|\leq E
\end{split}
\end{align}


\subsection{TTLP and LTLP}
