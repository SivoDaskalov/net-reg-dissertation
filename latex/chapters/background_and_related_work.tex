\chapter{Background and Related Work} \label{background}
This chapter briefly reviews the main concepts of linear regression. We describe in detail the various methods for penalized regression found in literature and used in this project.


\section{Linear regression}
Let us consider an entity with a number of scalar measurable (observable) properties, e.g. temperature, weight, dimensions. We can define a matrix $X$ of $n$ rows and $p$ columns, such that each column contains the observed values of a particular property and each row represents an independent observation of values for all properties. Let us also define a vector $y$ of length $n$ containing the corresponding observed values of an arbitrary property of interest.

Linear regression is a method for modeling the relationships between a scalar dependent (target) variable $y$ and a number of explanatory variables (predictors) $X_1,...,X_p$. It assumes that this relationship is linear and assigns a regression coefficient $\beta_i$ to each predictor $X_i$, as well as a constant (offset) term $\beta_0$. The linear regression model takes the form shown in Equation \ref{eq:lin_reg}
\begin{equation} \label{eq:lin_reg}
y_i = \beta_01+\beta_1X_{i1}+\beta_2X_{i2}+...+\beta_pX_{ip}+\epsilon_i, \quad for\ i=1,2,...,n
\end{equation}
where $\epsilon_i$ represents noise, capturing all external factors influencing the target values, such as inaccuracy of measurement. The error $\epsilon_i$ introduces cannot be predicted or reduced.


\section{Ordinary least squares estimation} \label{sec:olse}
Ordinary least squares (OLS) is a method of estimating the unknown parameters $\beta$ in a linear regression model. It aims to minimize the sum of squared deviations of the observed values from the model prediction (\ref{eq:rss}), also called residual sum of squares (RSS).
\begin{equation} \label{eq:rss}
L(\beta) = \sum_{i=1}^{N} (y_i - x_i^T\beta)^2
\end{equation}
The parameter estimate $\hat{\beta}$ for the linear regression model is obtained as shown in equation \ref{eq:beta_est} through the minimization of the objective function $S(\beta)$.
\begin{equation} \label{eq:beta_est}
\hat{\beta} = argmin_{\beta \in R}\ S(\beta) = L(\beta)
\end{equation}


\section{Penalized regression} \label{sec:pen_reg}
Penalized regression methods introduce a penalty $P(\beta)$ to the objective function $S(\beta)$ in addition to the loss function $L(\beta)$. $P$ penalizes values of the unknown parameters that are considered unrealistic in the current context, which is done to obtain a more meaningful estimation. One or more regularization parameters $\lambda_i$ can be used to balance the effect of any introduced penalties by scaling them. The general form of penalized regression is shown in Equation \ref{eq:pen_reg}. 
\begin{equation} \label{eq:pen_reg}
S(\beta) = L(\beta) + P(\beta)
\end{equation}


\subsection{Ridge regression} \label{sec:ridge}
Ridge regression \cite{hoerl1970ridge}, also called Tikhonov or L2 regularization, is used to penalize large values in the $\beta$ estimate. The penalty, shown in Equation \ref{eq:ridge}, causes the parameter estimates of the less important predictors to be shrinked, but remain non-zero. As a result, L2 regularization does not perform feature selection.
\begin{equation} \label{eq:ridge}
P(\beta) = \lambda\sqrt{\sum_{i=1}^{p}\beta_i^2}
\end{equation}


\subsection{Lasso} \label{sec:lasso}
The least absolute shrinkage and selection operator (LASSO) was introduced by Tibshirani in \cite{tibshirani1996regression}. It produces a sparse coefficient vector, whose remaining non-zero elements define a subset of the most relevant predictors. Model sparsity is especially important in high-dimensional problems, such as those arising when processing epigenetic data. The L1 penalty, shown in Equation \ref{eq:lasso}, performs both variable selection and regularization.
\begin{equation} \label{eq:lasso}
P(\beta) = \lambda\sum_{i=1}^{p}\left|\beta_i\right|
\end{equation}


\subsection{Elastic Net} \label{sec:enet}
The Elastic Net \cite{zou2005regularization}, suggested by Zou and Hastie, linearly combines the L1 (\ref{eq:lasso}) and L2 (\ref{eq:ridge}) penalties. This approach overcomes the individual limitations of the Lasso and Ridge methods. The elastic net penalty, shown in Equation \ref{eq:enet}, is adjusted by two hyperparameters $\lambda_1$ and $\lambda_2$, one for each of the two penalty terms.
\begin{equation} \label{eq:enet}
P(\beta) = \lambda_1\sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2\sqrt{\sum_{i=1}^{p}\beta_i^2}
\end{equation}


\section{Network-constrained regularization}
Various approaches for network-constrained regularization have been developed in recent years. They enable the use of prior knowledge in the form of a network in the parameter estimation process. This allows methods to consider known relationships between predictors. In the context of epigenetic research, prior knowledge could be provided as a gene network representing known interactions between genes. Biological knowledge about the predictors should lead to a better understanding of the data and improved (biological) meaningfulness of the results.

For all network-constrained regularization approaches presented in this section, we define the following notation:\\
Let us consider a network that is represented by a weighted graph $G = (V, E, W)$, where $V$ is the set of vertices corresponding to the $p$ predictors, $E$ is the set of edges and $W$ contains their corresponding weights. An edge between the vertices $u$ and $v$ is represented as $u \sim v$ and its edge weight is $w(u,v)$. Let us define the degree $d_v$ of a vertex $v$ as $d_v = \sum_{u \sim v}w(u,v)$.


\subsection{Grace} \label{sec:grace}
The first approach for network-constrained regularization was suggested by Li and Li \cite{li2008network}. The alias "Grace" is derived from the method's full name "GRAph Constrained Estimation". The penalty function, shown in Equation \ref{eq:grace}, contains two terms - an $L1$ penalty for variable selection and a second term that performs the network penalization.  
\begin{equation} \label{eq:grace}
P(\beta) = \lambda_1\sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2\sum_{u \sim v}\left(\frac{\beta_u}{\sqrt{d_u}}-\frac{\beta_v}{\sqrt{d_v}}\right)^2w(u,v)
\end{equation}
The penalty is designed to smooth the parameters $\beta$ over the gene network. This is achieved by penalizing the scaled difference of the coefficients between neighboring vertices in the network. The penalty encourages genes with a higher degree in the network (e.g. hub genes) to have larger coefficients.


\subsection{aGrace} \label{sec:agrace}
One drawback of the original Grace approach is that it performs poorly when the coefficients of two linked predictors have different signs. This scenario is feasible because one of the two genes could be negatively correlated with the target, in which case the coefficients of both genes will be penalized.  

Li and Li proposed a modification \cite{li2010variable} that performs adaptive graph-constrained regularization (aGrace) to solve this issue. It uses an initial coefficient estimate $\tilde{\beta}_v$ obtained through OLSE (\ref{sec:olse}) if $p<n$ or Elastic Net (\ref{sec:enet}) otherwise. The adaptive Grace penalty function is shown in Equation \ref{eq:agrace}.
\begin{equation} \label{eq:agrace}
P(\beta) = \lambda_1\sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2\sum_{u \sim v}\left(\frac{sign(\tilde{\beta}_u)\beta_u}{\sqrt{d_u}}-\frac{sign(\tilde{\beta}_v)\beta_v}{\sqrt{d_v}}\right)^2w(u,v),
\end{equation}
where the multiplier $sign(\tilde{\beta}_u) = \left\{ \begin{aligned} -1 &\quad if \: \tilde{\beta}_u < 0 \cr 1 &\quad otherwise \end{aligned} \right.$ adjusts the sign of each fraction as suggested by the initial estimate $\tilde{\beta}$.


\subsection{GBLasso} \label{sec:gblasso}
One concern regarding the adaptive grace (\ref{sec:agrace}) method is the difficulty to estimate the sign adjustment of all $\beta_i$, for which $\tilde{\beta}_i = 0$. To discard the need for this estimation, Pan et al. proposed an alternative approach \cite{pan2010incorporating}. The authors suggested the penalty function shown in Equation \ref{eq:gblasso_full}. 
\begin{equation} \label{eq:gblasso_full}
P(\beta) = \lambda 2^{1/\gamma'}\sum_{u \sim v}\left(\frac{|b_u|^\gamma}{w_u}+\frac{|b_v|^\gamma}{w_v}\right)^{1/\gamma},
\end{equation}
where $\gamma > 1$ and $\lambda > 0$ are hyperparameters and $\gamma'$ satisfies $\frac{1}{\gamma'}+\frac{1}{\gamma}=1$. The denominator $w_i$ is a weight function attributed to each node. Three types of weight functions, dependent on the node's degree $d_i$ and/or $\gamma$, were initially considered by the authors: $w_i = d_i^{(\gamma+1)/2}$, $w_i = d_i$ and $w_i = d_i^\gamma$. 

A simplification of the penalty function is presented in \cite{luo2012two}. The authors have selected a node weight function of $w_i = d_i^{\gamma/2}$ and the penalty sum multiplier $\lambda2^{1/\gamma'}$ has been modified to depend exclusively on $\lambda$. The simplified penalty function is shown in Equation \ref{eq:gblasso_simplified} and referred to with the alias GBLasso in this paper.
\begin{equation} \label{eq:gblasso_simplified}
P(\beta) = \lambda\sum_{u \sim v}
\left[\left(\frac{|b_u|}{\sqrt{d_u}}\right)^\gamma+
\left(\frac{|b_v|}{\sqrt{d_v}}\right)^\gamma\right]^{1/\gamma}
\end{equation}



\subsection{Linf and aLinf}

\subsubsection{Linf} \label{sec:linf}
Luo et al. \cite{luo2012two} continued researching the GBLasso method (\ref{sec:gblasso}). They noted that as $\gamma\rightarrow\infty$ the GBLasso penalty (\ref{eq:gblasso_simplified}) is transformed into Equation \ref{eq:linf_pen}. This penalty is linear and we denote the method as $L_\infty$ (Linf). 
\begin{equation} \label{eq:linf_pen}
P(\beta) = \lambda\sum_{u \sim v}\max\left(\frac{|\beta_u|}{\sqrt{d_u}},\frac{|\beta_v|}{\sqrt{d_v}}\right)
\end{equation}

The authors also suggest an equivalent formulation of the GBLasso-based regression as the following constrained minimization problem:
\begin{align} \label{eq:gblasso_constrained}
\begin{split}
S(\beta) = &\sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \\ 
&subject\ to\ \sum_{u \sim v}
\left[\left(\frac{|b_u|}{\sqrt{d_u}}\right)^\gamma+
\left(\frac{|b_v|}{\sqrt{d_v}}\right)^\gamma\right]^{1/\gamma}\leq C
\end{split}
\end{align}

Similarly, regression with the $L_\infty$ penalty can be equivalently defined as:
\begin{align} \label{eq:linf_constrained}
\begin{split}
S(\beta) = &\sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \\ 
&subject\ to\ \sum_{u \sim v}\max\left(\frac{|\beta_u|}{\sqrt{d_u}},\frac{|\beta_v|}{\sqrt{d_v}}\right)\leq C
\end{split}
\end{align}

\subsubsection{aLinf} \label{sec:alinf}
The authors suggest an additional modification to reduce bias in the parameter estimates of the standard Linf method. Similarly to \cite{li2010variable}, they propose a two-step approach using an initial parameter estimate $\tilde{\beta}$, obtained with the $L_\infty$ method. The adaptive penalty, denoted as $aL_\infty$ (aLinf), is shown in Equation \ref{eq:alinf_pen}.

\begin{equation} \label{eq:alinf_pen}
P(\beta) = \lambda\sum_{u \sim v}\left|\frac{sign(\tilde{\beta}_u)\beta_u}{\sqrt{d_u}}-\frac{sign(\tilde{\beta}_v)\beta_v}{\sqrt{d_v}}\right|
\end{equation}

The following constrained minimization problem can be defined to implement the $aL_\infty$ approach:
\begin{align} \label{eq:alinf_constrained}
\begin{split}
S(\beta) = &\sum_{i=1}^{n} (y_i - x_i^T\beta)^2 \\ 
&subject\ to\ \sum_{u \sim v}\left|\frac{sign(\tilde{\beta}_u)\beta_u}{\sqrt{d_u}}-\frac{sign(\tilde{\beta}_v)\beta_v}{\sqrt{d_v}}\right|\leq E
\end{split}
\end{align}


\subsection{TTLP and LTLP}
Kim et al. \cite{kim2013network} suggested two alternative network constrained regression methods based on the penalty shown in Equation \ref{eq:tlp_pen}. The first subpenalty is the $L_0$-loss for sparsest variable selection and unbiased parameter estimation proposed by Shen et al \cite{shen2012likelihood}. The second subpenalty encourages simultaneous selection or elimination of neighboring predictors in the network. the penalties are defined with the use of indicator functions notation explained in the following subsection.
\begin{equation} \label{eq:tlp_pen}
P(\beta) = \lambda_1 \sum_{i=1}^{p} [|\beta_i|\neq 0] + \lambda_2 \sum_{u \sim v} \left|\left[\frac{|\beta_u|}{w_u}\neq 0\right]-\left[\frac{|\beta_v|}{w_v}\neq 0\right]\right|
\end{equation}

\subsubsection{Indicator functions}
An indicator (characteristic) function is defined on a set $X$ and some subset $A \subset X$. The function indicates membership of elements in the subset $A$, having value of 1 for all elements of $A$ and value of 0 for all other elements in $X$. Formally, indicator functions are defined as follows:
\begin{equation}
[x \in A] = 1_A (x) = \left\{ 
\begin{aligned} 
1 & \quad if \quad x \in A \cr 
0 & \quad if \quad x \notin A
\end{aligned} \right.
for\ each\ x\ in\ X
\end{equation}
Note that the Iverson bracket notation $[P(x)]$ can be used equivalently to denote the indicator function of elements for which the condition $P$ is true.

\subsubsection{TTLP} \label{sec:ttlp}
Because the indicator function is not continuous, Shen et al. \cite{shen2012likelihood} proposed a truncated Lasso penalty (TLP) $J_\tau$ for a computational substitute. The TLP penalty, defined in Equation \ref{eq:tlp}, tends to $[|z|\neq 0]$ as $\tau \to 0^+$. The tuning parameter $\tau$ determines the degree of approximation.
\begin{equation} \label{eq:tlp}
J_\tau(|z|)=min\left(\frac{|z|}{\tau},1\right)
\end{equation}
Applying the TLP substitute to Equation \ref{eq:tlp_pen} produces the $TTLP_I$ penalty, shown in Equation \ref{eq:ttlp}, which uses TLP for both variable selection and grouping.
\begin{equation} \label{eq:ttlp}
P(\beta) = \lambda_1 \sum_{i=1}^{p} J_\tau|\beta_i| + \lambda_2 \sum_{u \sim v} \left|J_\tau\left(\frac{|\beta_u|}{w_u}\right)-J_\tau\left(\frac{|\beta_v|}{w_v}\right)\right|
\end{equation}

\subsubsection{LTLP} \label{sec:ltlp}
As an alternative to the TTLP, Kim et al. proposed a modification of their penalty using the Lasso for variable selection. The modified penalty, which the authors call $LTLP_I$, is shown in Equation \ref{eq:ltlp}.
\begin{equation} \label{eq:ltlp}
P(\beta) = \lambda_1 \sum_{i=1}^{p}\left|\beta_i\right| + \lambda_2 \sum_{u \sim v} \left|J_\tau\left(\frac{|\beta_u|}{w_u}\right)-J_\tau\left(\frac{|\beta_v|}{w_v}\right)\right|
\end{equation}
