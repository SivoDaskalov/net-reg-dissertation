\chapter{Hyperparameter Tuning} \label{tuning}
Many machine learning algorithms have one or more hyperparameters, whose role is to modify different aspects of the learning process. Their values are not estimated through use of the training data, but must be chosen prior to the start of the learning process. Choosing a set of suitable hyperparameter values for a learning algorithm, also called model selection, is an important step to ensure that the algorithm performs well and does not overfit the training data.

\section{Traditional approaches} \label{sec:trad_tuning}

\subsubsection{Grid search}
Grid search, also called parameter sweep, is commonly used to perform hyperparameter optimization. A predefined set of values is selected for each of the tuning parameters used by the learning algorithm. Models are then trained with each possible combination of tuning parameter values. All models are evaluated according to some performance metric. The combination of parameter values that produces the model performing best (minimizing/maximizing the performance metric) is chosen as optimal. The performance metric used typically in regression problems is the mean squared prediction error (MSE), calculated as shown in Equation \ref{eq:mse}.
\begin{equation} \label{eq:mse}
MSE = \frac{1}{n} \sum_{i=1}^{n} (\hat{y_i}-y_i)^2, 
\end{equation}
where $\hat{y_i}$ is the model's predicted value and $y_i$ is the true value of the target. 

\subsubsection{The validation set approach}
Estimating a model's MSE using the same data it was trained on is not truly indicative of the model's performance due to the possibility of overfitting. One way to measure a model's accuracy of prediction on unseen data is to calculate generalization error, also known as out-of-sample error. 

This could be done through the validation set approach by partitioning the available data into two mutually exclusive subsets called training and test (validation) sets. Models are then trained on the training subset of observations and their prediction MSE is calculated using the test set, also called holdout. However, this method has the following drawbacks \cite{james2013introduction}:
\begin{itemize}
	\item The validation estimate of the test error rate can vary greatly depending on the training-validation set partitioning
	\item The method makes inefficient use of the data as a significant part of the observations are never used for training
\end{itemize} 

\subsubsection{K-fold cross validation}
Cross validation \cite{james2013introduction, kohavi1995study} is a resampling method closely related to and addressing the drawbacks of the validation set approach. The often used k-fold cross-validation involves partitioning the data into $k$ subsets (folds). One of the folds is treated as a validation set and the model is trained on the remaining $k-1$ folds. The mean squared error for the fold $MSE_i$ is computed for the observations in the held-out fold $i$. The process is repeated $k$ times, each of the folds being held out once, and the k-fold CV estimate is obtained by averaging the estimates for the different folds as shown in Equation \ref{eq:cv}.
\begin{equation} \label{eq:cv}
MSE_{(k)} = \frac{1}{k} \sum_{i=1}^{k} MSE_i
\end{equation}

\subsubsection{Grid search minimizing the cross-validated MSE}
One traditional approach of performing model selection uses a grid search with cross-validated mean squared prediction error for a metric of model performance. The final model is then trained on the whole dataset using the optimal hyperparameter values that minimize the cross-validated MSE. This model selection approach is denoted as "CV-MSE" tuning in the latter chapters of this report.

\section{Orchestrated parameter tuning} \label{sec:orc_par_tun}

\subsection{Context and motivation}
In the context of this project multiple methods of performing regression are defined, each of them minimizing a different objective function. All approaches operate on the same training data and share a common goal to correctly identify the relationship between predictors and the target variable. 

Traditionally, we would tune the hyperparameters for each regression method independently before processing the desired data with the optimal parameter combinations for each method. Such an approach would not benefit from having an ensemble of regression approaches as they would all function completely independently. Our aim was to develop a hyperparameter optimization approach that would use this presence of multiple alternative approaches to perform simultaneous and cooperative parameter tuning.

\subsection{Inspiration}
Our novel hyperparameter tuning approach presented in Section \ref{sec:orc_meth} is inspired by a different kind of ensemble, that of a philharmonic orchestra. Much like the different regression methods, every musical instrument produces its own distinguishing sound even when playing the same melody. All of the various instruments complement each other and contribute in their own way to the skillful masterpiece that is a symphony. 

During a performance every musician needs to ensure that they are in synchronization with the rest of the orchestra. They need to be constantly aware of what the current general tempo and state of the melody across the whole ensemble is. In case of a mismatch, the performer needs to adjust their own way of playing their instrument to bring it back to synchronization with the other instruments.

The behavior of each regression method in our proposed hyperparameter tuning closely resembles these synchronizing adjustments made by orchestra members during a performance.

\subsection{Methodology} \label{sec:orc_meth}
We propose an alternative approach for simultaneous cooperative hyperparameter tuning that uses an ensemble of regression methods sharing a similar goal and input data. Our tuning method features an iterative algorithm that attempts to increase the similarity between parameter estimates of the various regression approaches at each step.

\subsubsection{Iterative procedure}
Initially, each of the methods is trained using some hyperparameter combination in their respective search grids, forming the method outputs for the "zero" iteration. The choice of this initial starting point is discussed in a subsection below.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{search_grid}
	\caption{Local search grid neighborhood, two tuning hyperparameters; $P_{t-1}$ is the parameter combination selected in iteration $t-1$, parameter values in the $P_t$ neighborhood are considered in iteration $t$}
	\label{fig:orc_tun_search_grid}
\end{figure}

The orchestrated tuning procedure of $k$ regression methods is defined below. The following sequence of steps is performed for each method $M_{i \in 1..k}$ in an arbitrary iteration $t$.
\begin{enumerate}
	\item \label{target_vector} Consider the coefficient vectors $\beta_{j, t-1}$ estimated by all other methods $M_{j \ne i}$ for the previous iteration $t-1$. A target coefficient vector $\beta_{tar, t}$ is created by calculating the mean coefficient value for each predictor estimated by the regression methods $M_{j \ne i}$.
	\item \label{cand_comb} Consider the candidate hyperparameter value combinations located in proximity (in the search grid) to the combination selected by the previous iteration $P_{t-1}$. An example of a set of candidate combinations is shown on Figure \ref{fig:orc_tun_search_grid} for a regression method with two tuning parameters. We train the current method $M_i$ using each of the candidate hyperparameter combinations.
	\item Consider the estimated coefficient vectors resulting from each of the candidate hyperparameter combinations defined in \ref{cand_comb}. The combination $P_t$ selected for the current iteration $t$ is the one that maximizes correlation between its estimated coefficient vector and the target vector $\beta_{tar, t}$ defined in \ref{target_vector}. The method's estimate $\beta_{i, t}$ for the current iteration is the one produced by the chosen candidate combination.
\end{enumerate}

This iterative procedure converges when any movement along the parameter grids is settled for all methods. The tuning stops when the selected hyperparameter combinations for the current iteration are identical to those of the previous one for all methods. The combinations of hyperparameter values selected in the last iteration are considered optimal for their respective regression methods.

If the parameter search grids for the various methods are coarse, fluctuation between two parameter combinations can occur, preventing convergence. A maximum number of iterations can be defined to force the completion of the tuning process, resulting in an approximate solution. 

The temporal relationships that occur between the various regression methods during the orchestrated hyperparameter tuning can be illustrated with the graph shown in Figure \ref{fig:orc_tun_struct}. This abstract structure of method interactions resembles a recurrent neural network.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{orchestrated_tuning_methodology}
	\caption{Orchestrated tuning abstract structure, three regression methods; $M1_{t-1}$ denotes the parameter estimates of method $1$ for iteration $t-1$; Each tuning iteration $t$ of a method uses the common input data and the parameter estimates of all other methods for the previous iteration}
	\label{fig:orc_tun_struct}
\end{figure}

\subsubsection{Choice of starting points for the search}
The choice of hyperparameter starting points in the search grids for the ensemble of regression methods directly affects the convergence state of the tuning process. This initial state of the system can be chosen in one of the following ways:

\begin{enumerate}
\item One approach could be to initialize the tuning process with hyperparameter values located in the centers of the search grids of all methods. Such initialization would allow the tuning procedure to decide which orthant of the search space to converge in. 
\item An alternative approach would be to start the tuning procedure from specific corners of the search grids. Depending on our desire to promote simpler or more complete models, the appropriate grid corner should be chosen, maximizing or minimizing the severity of imposed penalties.
\item The proposed orchestrated hyperparameter tuning could be combined with the traditional tuning approach discussed in Section \ref{sec:trad_tuning}. This can be done by obtaining the initial parameter values for the orchestrated tuning from a previously performed CV-MSE tuning. Such a combined tuning procedure could potentially benefit from the strengths of both tuning approaches - selection of models with good generalized prediction capabilities, but also coordinated for better agreement between regression methods.
\item A randomized orchestrated tuning procedure could be constructed by repeatedly performing the search with randomized starting points and collecting statistics from each convergence. The optimal hyperparameter combinations would be selected after analysis of the convergence statistics for all tuning executions.
\end{enumerate}

\subsection{Possible approach modifications}
A number of possible modifications for the basic orchestrated hyperparameter tuning approach are presented in this section. They have not been evaluated through simulations and should serve as suggestions for future work.

\subsubsection{Alternative criterion for comparison}
Instead of maximizing the overall estimate correlation in each iteration, a different criterion could be used. For example, we could consider the number of selected predictors by each regression method. In this scenario, hyperparameter values which lead to the selection of a number of variables closest to the mean would be chosen for each iteration. In this way the selection of similarly sized subsets of variables would be encouraged.
If more than one combination of parameter values causes an equal number of variables to be selected, a second criterion must be used in order to decide on a single combination. This additional criterion could be the correlation between estimates or a different metric suitable for the current context.

\subsubsection{Introduction of activation energy requirement for grid movement}
We can introduce a condition, resembling an activation energy requirement, to decide whether to move along the hyperparameter search grid or stay in the current position. For example, we define a requirement for improvement significance of at least $0.05$ in terms of overall estimate correlation. Movement to a new point in the search grid will occur only if a parameter combination is found that improves the overall estimate correlation by more than $0.05$ compared to the current location in the search grid.
This modification could significantly speed up the convergence of the parameter tuning process. Furthermore, it could prevent cyclic fluctuation between similarly performing parameter combinations and enable the procedure to finish successfully.

\subsubsection{Size of local grid search neighborhood}
The orchestrated tuning approach considers for each iteration the parameter combinations directly adjacent to the combination selected by the previous iteration, as shown in Figure \ref{fig:orc_tun_search_grid}. Instead, we can define a larger local neighborhood for the search grids of some or all of the regression methods.
This modification attempts to avoid convergence in points of local correlation maxima as it increases the breadth of search for each iteration. This allows more significant transitions along the search grids to be made, potentially reducing the number of iterations needed for convergence. 
Additionally, this modification can be combined with the idea of energy requirement by imposing a greater energy requirement for more distant search grid transitions.

\subsection{Discussion}
The orchestrated hyperparameter tuning approach presented in this chapter operates in the context of an ensemble of alternative methods sharing a common input and goals. In the case of regression, the approach aims to find combinations of hyperparameter values for the various methods that result in increased similarity of their estimated coefficients.

As previously discussed, the optimization focus is on the estimated predictor coefficients, not on the prediction error. This discards the need to perform cross validation, which could be valuable when working with computationally intensive regression methods. What is more, not optimizing the hyperparameters in terms of prediction error could result in reduction of overfitting.

The proposed hyperparameter tuning approach introduces cooperation between otherwise completely independent regression methods. This collective simultaneous tuning promotes consensus and agreement between the different methods. The resulting increased similarity of estimates simplifies the results interpretation because fewer conflicts between method outputs occur. 
